{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Words that don't add any meaning to an article (stopwords)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.read_csv('train.csv')\n",
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values with empty string\n",
    "news_df = news_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['content'] = news_df['author'] + ' ' + news_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_df.drop(columns='label', axis = 1)\n",
    "y = news_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a process of reducing a word to its root word:\n",
    "\n",
    "e.g. actor, actress, acting --> act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    # Only keeping alphabets in the content \n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    # Removing spaces and converting to lowercase\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    # Stemming the words that are not in stopwords\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    # Joining the list of words into a string\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['content'] = news_df['content'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_df['content'].values\n",
    "y = news_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=2)\n",
    "\n",
    "model_lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of training data: 0.9866%\n"
     ]
    }
   ],
   "source": [
    "# accuracy of training data\n",
    "\n",
    "X_train_prediction = model_lr.predict(X_train)\n",
    "print(f'Accuracy score of training data: {accuracy_score(X_train_prediction, y_train):.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of testing data: 0.9791%\n"
     ]
    }
   ],
   "source": [
    "# accuracy of testing data\n",
    "\n",
    "X_test_prediction = model_lr.predict(X_test)\n",
    "print(f'Accuracy score of testing data: {accuracy_score(X_test_prediction, y_test):.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Predicitive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Girl Asks Her 20 Boyfriends To Each Give Her an iPhone, Uses The Money To Buy a House</td>\n",
       "      <td>Eddy Lavine</td>\n",
       "      <td>Many Women Suffer From Fungal Infections And They Can Cure Them At Home Ξ [November 1, 2016] BLOG Girl Asks Her 20 Boyfriends To Each Give Her an iPhone, Uses The Money To Buy a House posted by Eddie We have heard the stories about Apple fanatics selling body parts and changing their passport in order to get the latest products from the company , but this latest story coming from China gives a different perspective on how you can use Apple devices as a tool. As a rather strange tool, in fact: an entrepreneurial Chinese woman is said to have convinced 20 men that she has been dating to buy her iPhones, which she would then sell and use the money as a downpayment for a house. The purchase of a house is probably the single biggest one-time spending in the lifetime of a person, and it could take long years of hard work, so it is indeed remarkable that the woman found the quick way to get there (however, not the most ethical one, arguably). Let’s make it clear: the story seems to have started on local Tian Ya Yi Du forum, where someone by the nickname Proud Qiaoba told the story of how her co-worker Xiaoli (not her real name) asked each of her 20 current boyfriends to buy her the new iPhone 7. How in the world is it possible to simulatenously date 20 people remains a mystery that gives this whole story a bit of a fairy tale aura, but knowing that China suffers from a terrible male-to-female ratio let’s assume that is somehow possible. Xiaoli, whose life remains veiled in secrecy, then sold all of the phones to a suspiciously particular place: mobile phone recycling site Hui Shou Bao, all for a total of 120,000 Chinese yuan, the equivalent of around $17,500. The woman then used the cash for a downpayment towards a countryhouse home. Xiaoli broke the news when she invited her co-workers for a house warming party. “Everyone in the office is talking about this now. Who knows what her boyfriends think now this news has become public.” From Qiaoba we also learn that Xiaoli “is not from a wealthy family. Her mum is a housewife and her dad is a migrant worker, and she is the oldest daughter. Her parents are getting old and she might be under a lot pressure hoping to buy them a house… But it’s still unbelievable that she could use this method!” \\nIt does indeed sound quite fantastical and made-up, and some have theorized that it’s all just a theory to popularize the local Hui Shou Bao phone re-seller (the company has confirmed that it had indeed sold 20 iPhones from a single person recently). Meanwhile, the whole story quickly started becoming popular in China: a ’20 mobiles for a house’ hashtag went trending on local microblog Weibo and has been used more than 13 million times. Reaction sway from admiration for Xiaoli’s boyfriend-getting and management skills to sheer awe and condemnation of her actions. Whatever it is, Xiaoli – if she even exists – is now said to shy away from the public eye and to refuse interviews from media. That’s actually not surprising… those 20 boyfriends surely would not be happy about her actions. Source:</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Select a random test sample\n",
    "random_test = test_df.sample(n=1)\n",
    "\n",
    "# Preprocess the content\n",
    "random_test['content'] = random_test['author'] + ' ' + random_test['title']\n",
    "\n",
    "random_test['content'] = random_test['content'].apply(stemming)\n",
    "random_test_X = random_test['content'].values\n",
    "\n",
    "# Transform the content using the vectorizer\n",
    "random_test_X = vectorizer.transform(random_test_X)\n",
    "\n",
    "# Make predictions\n",
    "prediction = model_lr.predict(random_test_X)\n",
    "\n",
    "# Create a table to display the prediction\n",
    "prediction_table = pd.DataFrame({'Title': random_test['title'],\n",
    "                                 'Author': random_test['author'],\n",
    "                                 'Text': random_test['text'],\n",
    "                                 'Prediction': ['Real' if prediction[0] == 0 else 'Fake']})\n",
    "\n",
    "# Display the table\n",
    "display(HTML(prediction_table.to_html(index=False)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
